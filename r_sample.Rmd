---
title: "r_sample"
author: "Michael D Evans"
date: "5/6/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr) 
library(here)
library(comprehenr)
```

R sample code
By Michael D. Evans
  In this sample, I generate some **simulated data** for a study where participants complete some surveys and play a game that measures risk assesment (game link below!). Participants will be in two groups, a  treatment group ("group1") and a no-treatment group ("group2").
  
  The hypotheses I am testing are whether participants in the treatment group are more accurately able to assess risk, whether there are differences in survey responses for the groups, and which items from the surveys are most strongly associated with performance.

Survey1: five facet mindfulness questionnaire (39 items)
Survey2: Barret impulsivity scale (30 items)
    
Behavioral Task: Ice Cream Game (try it out: www.mdevans.me/projects/icecream; adapted from BART task, Lejuez et al. 2002 http://www.impulsivity.org/measurement/BART)
    Your goal in the Ice Cream Game is to sell as much ice cream using as few cones as possible (e.g. sell as many scoops of ice cream as possible for each trial). Each scoop increases the reward value +1. At any point (even the first scoop), the ice cream can fall, and you cannot sell any- you receive 0 points and move onto the next trial. The risk of the scoops falling increases with each additional scoop, with it guranteed to fall by the 64th scoop (1/64 probability on the first scoop, 32/64 on the 32nd scoop, etc.). Optimal behavior would be to sell the ice cream when the probability of failure is as close to 50% as possible, in this case, at 32 scoops. Fewer scoops results in missing out on probable reward, and more scoops results in unneccesary risk.
    
Participants:
Group1 (treatment): 100 adults, 18-55
Group2 (control):100 adults, 18-55

Hypotheses:
Hyp1: group1 will show greater task learning and risk appraisal demonstrated with scores non-significantly different from optimal performance (32/64 = .5); group2 will show detrimental risk aversion demonstrated with scores signficantly below optimal.<br>
Hyp2: group1 performance scores will be statistically different (greater) than group2<br>
Hyp3: group1 will be statistically different from group2 in all questionnaires as following: higher for FFMQ, lower for BIS, lower for PTQ.

```{r set the stage}
n<-200 # set TOTAL number of subjects
n_groups<-2 # number of groups, subject numbers randomly assigned
participants <- sample.int(1e3, n) #assign random subject ids 1-1000
group1 <- participants[1:(n/2)] # first half to group1
group2 <- participants[((n/2)+1):n] # second half to group2
length(group1)==length(group2) && length(group1)==(n/2) # check lengths match up

# create main data table, populate some fields
# columns: subj, group, age, gender, ffmq1, ffmq2...bis1, bis2...ic_risk, ic_total, ic_mean, ic_rt 
df <- data.frame(
  id = c(group1,group2), # combine participants (could also just use participants)
  group = c(rep(1,(n/2)),rep(2,(n/2))), # assign group number
  age = runif(n,18,55) # generate ages, random sample uniform dist range 18-55
)

# other fields require more nuanced distributions

# gender
# we'll sample gender from categorical dist (.49male, .49female, .2non-binary/other)
# well use random uni dist 0-1, 0-.49 = m, .49-.98 = f, >=.98 = nb
# since we're using random numbers, our sample may not perfectly reflect our simulated population e.g. may have 0 non-binary individuals
m<-0.49 # e.g 49%
f<-0.49
nb<-1-m-f # remainder *note rule: m+f<=1
gender = runif(n) # generate n-number values 0-1 random uniform dist.

for (ii in 1:n){ # counter to go through each gender item
  if (gender[ii]<m){ # if value below male threshold
    gender[ii]<-'m'
    next # skip to next counter item otherwise value overwritten
  }else if (gender[ii]<m+f){ # if value greater than male,but less than male+female
    gender[ii]<-'f'
    next
  }else{ # <= m+f
    gender[ii]<-'nb'
  }
}
df$gender = gender # add to our dataframe

#ice cream game
# since we are simulating data, let's simulate are hypotheses are true
# so..
# group1 we'll random sample from a norm dist. m=0.5 sd=0.08
# group2 random norm m=0.3 sd=0.15
m_g1 =32 # mean, group 1
sd_g1 = 2.4 # sd, group 1
m_g2 = 21.6
sd_g2 = 3.2
df$ic_scoops = c(rnorm((n/2),m_g1,sd_g1),rnorm((n/2),m_g2,sd_g2)) # normal dist, add to df
plt<-ggplot(data=df, aes=(x=ic_scoops))+ # plot as histogram to check distributions
  geom_histogram(mapping=aes(x=ic_scoops, fill=as.factor(group)),binwidth=3)
print(plt) # display plot

# any reason to think reaction times would be different between groups?
# if not, just assign from norm. dist w reasonable time (fast game, so expect low rt)
m_rt<-400 
sd_rt<-50
df$ic_rt = rnorm(n,m_rt,sd_rt) # normal dist., add to dataframe
plt<-ggplot(data=df)+
  geom_histogram(mapping=aes(x=ic_rt,fill=as.factor(group)),binwidth=50)
print(plt)
df
```


```{r}
## survey data
# first survey is five facet mindfulness questionnaire, 39 items w 5 factors 

# we hypothesized that treatment (group1) would score higher than control (group2), so we'll set anchors for those as well. 
# item responses range 1-5
m_ffmq_g1 <- 3.5 # mean ffmq score for group1
sd_ffmq_g1 <- 1 # sd ffmq score, group1

m_ffmq_g2 <- 2.5
sd_ffmq_g2 <- 1

# we'll generate data such that a factor analysis should reveal the factors by setting different anchors for each factor, and generative values for associated questions from those anchors
# factor: factor_name(factor_items)
# factor1: observing (1, 6, 11, 15, 20, 26, 31, 36)
obs_items = c(1,6,11,15,20,26,31,36) # we'll use later when generating data matrices
# factor2: describing (2, 7, 12R, 16R, 22R, 27, 32, 37) R = reverse scoring, can subtract upper limit to score (e.g 6; response:1, score:6-1=5)
desc_items = c(2,7,12,16,22,27,32,37)
# factor3: awareness (5R, 8R, 13R, 18R, 23R, 28R, 34R, 38R)
aware_items = c(5,8,13,18,23,28,34,38)
# factor4: nonjudging (3R, 10R, 14R, 17R, 25R, 30R, 35R, 39R)
nj_items = c(3,10,14,17,25,30,35,39)
# factor5: nonreactivity (4, 9, 19, 21, 24, 29, 33)
nr_items = c(4,9,19,21,24,29,33)

# preallocate matrix for score
ffmq <- matrix(data=NA,nrow=n,ncol=39) # 39 survey items

minMax <- function(m,floor=1,ceiling=5){
# function to set floor and ceiling values in a matrix
  m[m<floor]<-floor # set any values less than floor to floor
  m[m>ceiling]<-ceiling # same for ceiling
  return(m)
}

removeItems <- function(m,factor_items){
# function to remove non-factor items from matrix
  m_vec <- to_vec(for(i in 1:39) # create vector for item placement,
    if(i %in% factor_items) 1 else 0)  # 1s in question index associated w factor, else NA
  
  for (ii in 1:length(m_vec)){ # go through binary vector representing factor items
    m[,ii]<-m[,ii]*m_vec[ii] #  if 0, drop column, if 1, keep column as is
  }
  return(m)
}

gen_factor_data <-function(factor_items,print_plot=FALSE){
# function to generate ffmq factor data
# input: factor indicies
# output: 200x39 matrix of factor item responses (non-factor filled w 0s)
#   rows 1:100: group1
#   rows 101:200: group2
  m <- c(rnorm(length(group1),m_ffmq_g1,sd_ffmq_g1), # generate mean factor score for group1
           rnorm(length(group2),m_ffmq_g2,sd_ffmq_g2)) # and group2
  m = matrix(rnorm(39*n,m,1),n,39) # generate matrix using random means (static per participant) 
  
  # using this method, we'll get some float values instead of integer
  mode(m)<-'integer' # set internal storage mode to integer (truncates)
  
  # we'll also run into values outside our range of 1-5
  m<-minMax(m)
  
  # good time to print some histograms to make sure distribution is as-expects
  if(print_plot==TRUE){
    hist(m[1:100,]) # first 100 rows e.g group1
    abline(v=m_ffmq_g1,col='red') # plot group1 population mean
    
    hist(obs[101:200,])# group2
    abline(v=m_ffmq_g2,col='red')
  }
  
  # get rid of non-factor columns
  m<-removeItems(m,factor_items)
  
  # my sample is slightly lower than defined means - due to changing data type to int instead of rounding, but overall looks good!
  # remove non-factor items
  return(m)
}

# generate factor item responses
obs <- gen_factor_data(obs_items)
desc <- gen_factor_data(desc_items)
aware <- gen_factor_data(aware_items)
njudge <- gen_factor_data(nj_items)
nreact <- gen_factor_data(nr_items)

# as is, all factors have the same means since they were generated w same population values
# let's assume some variability between factors, and for fun, say that the treatment
# works particularly well for non-judgement and non-react

# to make factors stand out, we can shift the means
obs<-obs-1 # lower all obs scores by 1
obs<-minMax(obs) # reset floor/ceiling
obs<-removeItems(obs,obs_items) # 1 added to 0 values, remove non-factor columns

njudge[1:100,]<-njudge[1:100,]+1 # increase treatment group non-judge by 1
njudge<-minMax(njudge)
njudge<-removeItems(njudge,nj_items)

nreact[1:100,]<-nreact[1:100,]+1 # increase tx group nreact 
nreact<-minMax(nreact)
nreact<-removeItems(nreact,nr_items)

aware[101:200,]<-aware[101:200,]+1 # increase control group aware (now ~= to tx group)
aware<-minMax(aware)
aware<-removeItems(aware,aware_items)

desc[101:200,]<-desc[101:200,]+1 # increast ctrl group desc (now ~= to tx group)
desc<-minMax(desc)
desc<-removeItems(desc,desc_items)
# since we counter-balanced updating values, overall mean for groups should be the same
# so simply comparing group1 v group2 should show sig diff.
# when we explore, we should be able to see which factors contribute most to this diff
# since we set some factors equal to others, I suspect our factor analysis will NOT
# reveal 5 factors (ones w ~equal means likely to correlate -> lumped into same factor)

ffmq = data.frame(obs+desc+aware+njudge+nreact) # combine to create final ffmq data frame

# add to main df
df$ffmq = ffmq
```

Now lets pretend that some of these traits are related to task performance- since we want the tx group to stand out, we can focus on the factors where we said tx was most effective: non-judgement (nj) and non-react (nr).

we know typically people under-perform in the task, which makes generating the data a little easier- we can simply set that people who are highest in non-judge+non-react have a mean of 32 and a small sd

ic = normal dist w mean=[8+(12*%nr)+(12*%nj)] and sd=[3/(0.5+(.25*%nj)+(0.25*%nr))]

Using this formula, individuals with perfect nj and nr scores will have a mean of 32 and a sd of 3 ( mean = 8+12*1+12*1 = 32 ; sd = 3/(.5+.25*1+.25*1) = 3/1 = 3)
Those with a 0 score will have a mean of 8 and a sd of 6 (mean = 8+12*0+12*0 = 8 ; sd = 3/(.5+.25*0+.25*0) = 3/.5 = 6)

Since these are the factors where the two groups differ most, we shouold see a main group effect. Then, when we do a factor analysis to see which factors best predict performance, we should see that it was the factors that the treatment was most influential on - nj and nr. These two factors share group means, so they will likely be lumped together in the factor analysis.

It is also a good idea to replicate some typical, known behaviors. For example, we know age is generally positively correlated with risk aversion - older people tend to be more cautious in these kinds of games. 

We can incorporate a linear expression of age in our construction of means to acheive this:
mean=[(8-((age-min(age)/max(age)-min(age))/.2)+(12*%nr)+(12*%nj)]

Expressing age as (age-min(age)/max(age)-min(age)) puts age in a 0-1 range
the youngest participants will have a base of 8 (8-0/.2) and the oldest a base of 3 (8-1/.2 = 8-5)




```{r scoops echo=FALSE}

# calculate nj and nr scores for each row
calcScoops = function(vec,age){
  
  # get response for each nj_item, normalize by max (n items * max valid response (5))
  nj_score = sum(to_vec(for(i in 1:length(vec)) if (i %in% nj_items) vec[i])) /(length(nj_items)*5)
  nr_score = sum(to_vec(for(i in 1:length(vec)) if (i %in% nr_items) vec[i])) /(length(nr_items)*5)
  
  mean = (8-((age-18)/(55-18))/.2)+(14*nj_score)+(14*nr_score) # calculate mean using defined equation
  sd = 3/(.5+(.25*nj_score)+(.25*nr_score)) #  and standard deviation
  
  scoops = rnorm(1, mean, sd) # pull from distribution
  
  
  # don't want negative (or even very low) values!
  
  if(scoops<4){scoops=scoops+4}
  
  return(scoops)
}

# before using ffmq data (just pre-set group means)
plt<-ggplot(data=df, aes=(x=ic_scoops))+ # plot as histogram to check distributions
  geom_histogram(mapping=aes(x=ic_scoops, fill=as.factor(group)),binwidth=3)
print(plt) # display plot

for (row in 1:nrow(df)){
  vec = df$ffmq[row,]
  age = df$age[row]
  df$ic_scoops[row]=calcScoops(vec,age)
}

# after
plt<-ggplot(data=df, aes=(x=ic_scoops))+ # plot as histogram to check distributions
  geom_histogram(mapping=aes(x=ic_scoops, fill=as.factor(group)),binwidth=3)
print(plt) # display plot

# while overall the data may not look too different - both groups are still normal in shape and easily distinguishable from one another, the main difference is just lower means (slightly wider sd's), these new distributions are generated from actual data, so it should give us some interesting things to find during analysis!

# save as a csv and move onto analysis!
write.csv(df,paste(here(),'/ffmq_ic_data.csv',sep=''))

  

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
